{
	"Import OpenVINO": {
		"prefix": "ov:import",
		"body": [
			"from openvino.inference_engine import IECore"
		],
		"description": "Import OpenVINO package"
	},
	"Initialize Inference Engine": {
		"prefix": "ov:core",
		"body": [
			"ie = IECore()"
		],
		"description": "Initialize Inference Engine with IECore()"
	},
	"Inference Engine Config ": {
		"prefix": "ov:config",
		"body": [
			"ie.set_config(${1:config[${2:device}]},${2:device})"
		],
		"description": "set Inference Engine configuration"
	},
	"Available_devices": {
		"prefix": "ov:device",
		"body": [
			"devices = ie.available_devices,",
			"for device in devices:",
				"\tdevice_name = ie.get_metric(device_name=device, metric_name=\"FULL_DEVICE_NAME\")",
				"\tprint(f\"{device}: {device_name}\")"
		],
		"description": "Show available_devices"
	},
	"Loading a Model": {
		"prefix": "ov:loadnetwork",
		"body": [
			"net = ie.read_network(model=\"${1:.xml}\")",
			"exec_net_onnx = ie.load_network(network=${1:net_onnx}, device_name=\"${2:CPU}\", num_requests=${3:1})"
		],
		"description": "Reading and Loading a Model"
	},
	"Model Input": {
		"prefix": "ov:inputinfo",
		"body": [
			"input_layer = next(iter(${1:net.input_info}))",
			"print(f\"input layout: {${1:net.input_info}[input_layer].layout}\")",
			"print(f\"input precision: {${1:net.input_info}[input_layer].precision}\")",
			"print(f\"input shape: {${1:net.input_info}[input_layer].tensor_desc.dims}\")"
		],
		"description": "Get model input information"
	},
	"Model Output": {
		"prefix": "ov:outputinfo",
		"body": [
			"output_layer = next(iter(${1:net.outputs}))",
			"print(f\"output layout: {${1:net.outputs}[output_layer].layout}\")",
			"print(f\"output precision: {${1:net.outputs}[output_layer].precision}\")",
			"print(f\"output shape: {${1:net.outputs}[output_layer].shape}\")"
		],
		"description": "Get model output information"
	},
	"Do sync inference": {
		"prefix": "ov:sync_infer",
		"body": [
			"result = exec_net.infer({${1:input_layer}: ${2:input_data}})",
			"output = result[${2:output_layer}]"
		],
		"description": "Doing sync inference"
	},
	"Do async inference": {
		"prefix": "ov:async_infer",
		"body": [
			"exec_net.request[${1:0}].async_infer(inputs={${2:input_layer}: ${3:input_data}})",
			"exec_net.requests[${1:0}].wait(-1)==0",
			"result = exec_net.requests[${1:0}].output_blobs[${4:output_layer}].buffer"
		],
		"description": "Doing async inference and waiting for the result is ready"
	},
	"Change model input shape": {
		"prefix": "ov:reshape",
		"body": [
			"new_shape = ${1:(1, 3, 544, 544)}",
			"${2:net}.reshape({${3:input_layer}: new_shape})"
		],
		"description": "Change model input shape"
	},
	"Change model batch size": {
		"prefix": "ov:batchsize",
		"body": [
			"${1:net}.batch_size = ${2:2}"
		],
		"description": "set model's batch_size"
	},
	"Caching a Model": {
		"prefix": "ov:caching",
		"body": [
			"cache_path = Path(${1:\"model/model_cache\"})",
			"ie.set_config({\"CACHE_DIR\": str(cache_path)}, device_name=${2:CPU})"
		],
		"description": "create a model_cache directory as a subdirectory of model, where the model will be cached for the specified device"
	}
}