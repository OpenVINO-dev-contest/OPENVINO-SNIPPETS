{
	"Sample Code for Image Classification": {
		"prefix": "ov:example",
		"body": [
			"import cv2",
			"import numpy as np",
			"from openvino.inference_engine import IECore",
			"",
			"ie = IECore()",
			"net = ie.read_network(",
    		"\tmodel=${1:'model/v3-small_224_1.0_float.xml'}, weights=${2:'model/v3-small_224_1.0_float.bin'}",
			")",
			"exec_net = ie.load_network(network=${1:net}, device_name=${2:'CPU'})",
			"",
			"input_key = next(iter(exec_net.input_info))",
			"output_key = next(iter(exec_net.outputs.keys()))",
			"",
			"# The MobileNet network expects images in RGB format",
			"image = cv2.cvtColor(cv2.imread(filename=${1:'data/coco.jpg'}), code=cv2.COLOR_BGR2RGB)",
			"# resize to MobileNet image shape",
			"input_image = cv2.resize(src=image, dsize=(224, 224))",
			"# reshape to network input shape",
			"input_image = np.expand_dims(input_image.transpose(2, 0, 1), 0)",
			"",
			"result = exec_net.infer(inputs={input_key: input_image})[output_key]",
			"result_index = np.argmax(result)",
			"",
			"# Convert the inference result to a class name.",
			"imagenet_classes = open(${1:'utils/imagenet_2012.txt'}).read().splitlines()",
			"# The model description states that for this model, class 0 is background,",
			"# so we add background at the beginning of imagenet_classes",
			"imagenet_classes = ['background'] + imagenet_classes",
			"imagenet_classes[result_index]"
		],
		"description": "Import OpenVINO package"
	},
	"Import OpenVINO": {
		"prefix": "ov:import",
		"body": [
			"from openvino.inference_engine import IECore"
		],
		"description": "Import OpenVINO package"
	},
	"Initialize Inference Engine": {
		"prefix": "ov:core",
		"body": [
			"ie = IECore()"
		],
		"description": "Initialize Inference Engine with IECore()"
	},
	"Set throughput streams ": {
		"prefix": "ov:streams",
		"body": [
			"config={}",
			"device=${1:\"CPU\"}",
			"key = device + ${2:\"THROUGHPUT_STREAMS\"}",
			"config[device] = {}",
			"config[device][key] = ${3:\"2\"}",
			"ie.set_config(config[device],device)"
		],
		"description": "set throughput streams"
	},
	"Available devices": {
		"prefix": "ov:device",
		"body": [
			"devices = ie.available_devices",
			"for device in devices:",
				"\tdevice_name = ie.get_metric(device_name=device, metric_name=\"FULL_DEVICE_NAME\")",
				"\tprint(f\"{device}: {device_name}\")"
		],
		"description": "Show available_devices"
	},
	"Loading a Model": {
		"prefix": "ov:loadnetwork",
		"body": [
			"net = ie.read_network(model=\"${1:.xml}\")",
			"exec_net = ie.load_network(network=net, device_name=\"${2:CPU}\", config=${3:config}, num_requests=${4:2})"
		],
		"description": "Reading and Loading a Model"
	},
	"Model Input": {
		"prefix": "ov:inputinfo",
		"body": [
			"input_layer = next(iter(${1:net.input_info}))",
			"print(f\"input layout: {${1:net.input_info}[input_layer].layout}\")",
			"print(f\"input precision: {${1:net.input_info}[input_layer].precision}\")",
			"print(f\"input shape: {${1:net.input_info}[input_layer].tensor_desc.dims}\")"
		],
		"description": "Get model input information"
	},
	"Model Output": {
		"prefix": "ov:outputinfo",
		"body": [
			"output_layer = next(iter(${1:net.outputs}))",
			"print(f\"output layout: {${1:net.outputs}[output_layer].layout}\")",
			"print(f\"output precision: {${1:net.outputs}[output_layer].precision}\")",
			"print(f\"output shape: {${1:net.outputs}[output_layer].shape}\")"
		],
		"description": "Get model output information"
	},
	"Do sync inference": {
		"prefix": "ov:sync_infer",
		"body": [
			"result = exec_net.infer({${1:input_layer}: ${2:input_data}})",
			"output = result[${2:output_layer}]"
		],
		"description": "Doing sync inference"
	},
	"Do async inference": {
		"prefix": "ov:async_infer",
		"body": [
			"exec_net.requests[${1:0}].async_infer(inputs={${2:input_layer}: ${3:input_data}})",
			"if exec_net.requests[${1:0}].wait(-1)==0:",
			"\tresult = exec_net.requests[${1:0}].output_blobs[${4:output_layer}].buffer"
		],
		"description": "Doing async inference and waiting for the result is ready"
	},
	"Change model input shape": {
		"prefix": "ov:reshape",
		"body": [
			"new_shape = ${1:(1, 3, 544, 544)}",
			"${2:net}.reshape({${3:input_layer}: new_shape})"
		],
		"description": "Change model input shape"
	},
	"Change model batch size": {
		"prefix": "ov:batchsize",
		"body": [
			"${1:net}.batch_size = ${2:2}"
		],
		"description": "set model's batch_size"
	},
	"Caching a Model": {
		"prefix": "ov:caching",
		"body": [
			"cache_path = Path(${1:\"model/model_cache\"})",
			"ie.set_config({\"CACHE_DIR\": str(cache_path)}, device_name=${2:CPU})"
		],
		"description": "create a model_cache directory as a subdirectory of model, where the model will be cached for the specified device"
	}
}