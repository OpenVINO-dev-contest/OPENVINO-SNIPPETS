{
	"Sample Code for Image Classification": {
		"prefix": "ov:example",
		"body": [
			"import cv2",
			"import numpy as np",
			"from openvino.runtime import Core",
			"",
			"ie = Core()",
			"model = ie.read_model(model=${1:'model/v3-small_224_1.0_float.xml'}}",
			"compiled_model = ie.compile_model(model=${1:model}, device_name=${2:'CPU'})",
			"",
			"input_layer = next(iter(compiled_model.outputs))",
			"output_layer = next(iter(compiled_model.inputs))",
			"",
			"# The MobileNet network expects images in RGB format",
			"image = cv2.cvtColor(cv2.imread(filename=${1:'data/coco.jpg'}), code=cv2.COLOR_BGR2RGB)",
			"# resize to MobileNet image shape",
			"input_image = cv2.resize(src=image, dsize=(224, 224))",
			"# reshape to network input shape",
			"input_image = np.expand_dims(input_image.transpose(2, 0, 1), 0)",
			"",
			"result = compiled_model(inputs=[input_image])[output_layer]",
			"result_index = np.argmax(result)",
			"",
			"# Convert the inference result to a class name.",
			"imagenet_classes = open(${3:'utils/imagenet_2012.txt'}).read().splitlines()",
			"# The model description states that for this model, class 0 is background,",
			"# so we add background at the beginning of imagenet_classes",
			"imagenet_classes = ['background'] + imagenet_classes",
			"imagenet_classes[result_index]"
		],
		"description": "Image Classification Example"
	},
	"Import OpenVINO": {
		"prefix": "ov:import",
		"body": [
			"from openvino.runtime import Core"
		],
		"description": "Import OpenVINO library"
	},
	"Reshape dynamic input": {
		"prefix": "ov:dynamic",
		"body": [
			"input_shape[3] = Dimension(-1)",
			"net.reshape({input_layer: input_shape})"
		],
		"description": "Declare the dimension of dynamic input shape"
	},
	"Initialize Inference Engine": {
		"prefix": "ov:core",
		"body": [
			"ie = Core()"
		],
		"description": "Initialize OpenVINO with Core()"
	},
	"Available devices": {
		"prefix": "ov:device",
		"body": [
			"devices = ie.available_devices",
			"for device in devices:",
				"\tdevice_name = ie.get_property(device_name=device, name=\"FULL_DEVICE_NAME\")",
				"\tprint(f\"{device}: {device_name}\")"
		],
		"description": "Show available_devices"
	},
	"Loading a Model": {
		"prefix": "ov:loadnetwork",
		"body": [
			"model = ie.read_model(model=\"${1:.xml}\")",
			"compiled_model = ie.compile_model(model=model, device_name=\"${2:CPU}\")"
		],
		"description": "Reading and Loading a Model"
	},
	"Model Input": {
		"prefix": "ov:inputinfo",
		"body": [
			"model.inputs[0].any_name",
			"input_layer = next(iter(model.inputs))",
			"print(f\"input precision: {input_layer.element_type}\")",
			"print(f\"input shape: {input_layer.shape}\")"
		],
		"description": "Get model input information"
	},
	"Model Output": {
		"prefix": "ov:outputinfo",
		"body": [
			"model.outputs[0].any_name",
			"output_layer = next(iter(model.outputs))",
			"print(f\"output precision: {output_layer.element_type}\")",
			"print(f\"output shape: {output_layer.shape}\")"
		],
		"description": "Get model output information"
	},
	"Do sync inference": {
		"prefix": "ov:sync_infer",
		"body": [
			"request = compiled_model.create_infer_request()",
			"request.infer(inputs={input_layer.any_name: input_data})",
			"result = request.get_output_tensor(output_layer.index).data"
		],
		"description": "Doing sync inference"
	},
	"Infer request queue": {
		"prefix": "ov:queue",
		"body": [
			"def callback(infer_request: InferRequest, ${1:userdata}) -> None:",
				"\tpredictions = next(iter(infer_request.results.values()))",
			"infer_queue = AsyncInferQueue(${2:compiled_model}, ${3:4})",
			"infer_queue.set_callback(callback)",
			"infer_queue.start_async(${4:{0: input_tensor}}, ${1:userdata})",
			"infer_queue.wait_all()"
		],
		"description": "Create infer request queue"
	},
	"Change model input shape": {
		"prefix": "ov:reshape",
		"body": [
			"new_shape = PartialShape(${1:[1, 3, 544, 544]})",
			"model.reshape({input_layer.any_name: new_shape})"
		],
		"description": "Change model input shape"
	},
	"Caching a Model": {
		"prefix": "ov:caching",
		"body": [
			"cache_path = Path(${1:\"model/model_cache\"})",
			"enable_caching = True",
    		"config_dict = {\"CACHE_DIR\": ${2:str(cache_path)}}", 
			"compiled_model = ie.compile_model(model=model, device_name=device_name, config=config_dict)"
		],
		"description": "create a model_cache directory as a subdirectory of model, where the model will be cached for the specified device"
	},
	"Preprocessing API": {
		"prefix": "ov:ppp",
		"body": [
			"ppp = PrePostProcessor(model)",
			"ppp.input().tensor()",
        		"\t.set_spatial_static_shape(h,w)",
        		"\t.set_layout(${1:Layout('NHWC')})", 
			"inputs = model.inputs",
			"ppp.input().model().set_layout(${2:Layout('NCHW')})",
			"ppp.input().preprocess()",
    			"\t.resize(ResizeAlgorithm.RESIZE_LINEAR,224,224)",
    			"\t.mean([0.485, 0.456,0.406])",
    			"\t.scale([0.229, 0.224, 0.225])",
			"ppp.output().tensor().set_element_type(${3:Type.f32})",
			"model = ppp.build()"
		],
		"description": "Integrate Preprocessing steps into execution graph"
	}
}